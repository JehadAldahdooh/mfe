---
title: "mfe: Meta-Features Extractor for Meta-Learning"
author: "Adriano Rivolli and Luis Paulo F. Garcia"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Introduction

The **mfe** package is designed to extract meta-features from datasets. The 
meta-features can be understood as characterization measures able to describe 
datasets to support recommendation systems based on Meta-learning (MtL). The 
package contains the standard and the state of the art characterization measures
 with the goal to improve the MtL experiments and also guide the complexity 
dataset understanding. In the current version, only classification problems are 
supported, however, we plan support regression datasets in the future versions. 

The meta-features are designed to extract general properties of datasets and 
provide evidences about the performance of algorithms in MtL recomendation 
systems. These measures must be able to predict, with a low computational cost, 
the performance of these algorithms. The measures used in MtL can be divided 
into six groups:

* **General**: General information related to the dataset, also known as simple 
measures, such as number of instances, attributes and classes.
* **Statistical**: Standard statistical measures to describe the numerical 
properties of a distribution of data.
* **Discriminant**: Measures computed using the discriminant analysis.
* **Information-theoretic**: Particularly appropriate to describe discrete 
(categorical) attributes and thei relationship with the classes.
* **Decision Tree Model-based**: Measures desined to extract characteristics 
like the depth, the shape and size of a Decision Tree model induced from a 
dataset.
* **Landmarking**: Represents the performance of some simple and efficient 
learning algorithms.

In the following sections we will briefly introduce how to use the **mfe** 
package to extract all the measures using stadard methods as well as to extract 
specific measures using methods for each measure groups. Once the package is 
loaded, the vignette is also available inside R with the command 
`browseVignettes`.

# Extracting meta-features

The standard way to extract meta-features is using the `metafeatures` methods. 
The method can be usage by a symbolic description of the model (formula) or by 
a data frame. The parameters are the dataset and the group of measures to be 
extracted. To extract all the measures, the parameter `group` needs to be set 
as `all`. For instance:

```{r}
library(mfe)

## Extract all measures using formula
iris.info <- metafeatures(Species ~ ., iris, groups="all")

## Extract all measures using data frame
iris.info <- metafeatures(iris[1:4], iris[,5], groups="all")

## Extract general, statistical and information-theoretic measures
iris.info <- metafeatures(Species ~ ., iris, 
                          groups=c("general", "statistical", "infotheo"))

```

Several measures return more than one value. To agregate them, post processed 
methods can be used. It is possible to compute min, max, mean, median, kurtosis,
 standard deviation, among others. The default methods are the `mean` and the 
 `sd`. For instance:## Landmarking## Landmarking


```{r}
## Compute all measures using min, median and max 
iris.info <- metafeatures(Species ~ ., iris, summary=c("min", "median", "max"))
                          
## Compute all measures using quantile
iris.info <- metafeatures(Species ~ ., iris, summary="quantile")
```

# Extracting meta-features by group

To customize the measure extraction, is necessary to use specific methods for 
each group of measures. For instance, `mf.general` and `mf.statistical` compute 
the general and the statistical measures, respectively. The folloing examples 
illustrate this cases:

```{r}
## Extract two statistical measures
stat.iris <- mf.statistical(Species ~ ., iris, 
                            features=c("correlation", "variance"))

## Extract two discriminant measures
disc.iris <- mf.discriminant(Species ~ ., iris, 
                             features=c("cancor", "cancor.fract"))

## Extract the histogram for the correlation measure
hist.iris <- mf.statistical(Species ~ ., iris, 
                            features="correlation", summary="hist")
```

Different from the `metafeatures` method, these methods receive a parameter 
called `features`, to define which features is required, and return a list 
instead of a numeric vector. In additional, some groups can be customized using
additional arguments.

There are six measure groups which can be either general information about the 
dataset, statistical information, discriminant analysis measures, descriptors 
about information theoretical, measures desined to extract characteristics about
 the Decision Tree (DT) model based or landmarks which represent the performance
 of simple algorithms applied to the dataset. The folloing example show the 
available groups:

```{r}
## Show the the available groups
ls.metafeatures()
```

## General

These are the most simple measures for extracting general properties of the 
datasets. For instance, `nattribute` and `nclasse` are the total number of 
attributes in the dataset and the number of output values (classes) in the 
dataset, respectively. To list the measures of this group use `ls.general()`. 
The following examples illustrate these measures:

```{r}
## Show the the available general measures
ls.general()

## Extract all general measures
general.iris <- mf.general(Species ~ ., iris, features="all")

## Extract two general measures
mf.general(Species ~ ., iris, features=c("nattribute", "nclasse"))
```

The general measures return a list named by the requested measures. The 
`post.processing` methods are not applied in these measures since they return 
simple values.

## Statistical
Statistical meta-features are the standard statistical measures to describe the 
numerical properties of a distribution of data. As it requires only numerical 
attributes, the categorical data are transformed to numerical. For instance, 
`correlation` and `skewness` are the absolute correlation between each pair of 
attributes and the skewness of the numeric attributes in the dataset, 
respectively. To list the measures of this group use `ls.statistical()`. 
The following examples illustrate these measures:

```{r}
## Show the the available statistical measures
ls.statistical()

## Extract all statistical measures
stat.iris <- mf.statistical(Species ~ ., iris, features="all")

## Extract two statistical measures
mf.statistical(Species ~ ., iris, features=c("correlation", "skewness"))

```

The statistical group requires an aditional parameter called `by.class`. The 
default is `by.class=TRUE` which means that the meta-features are computed over 
the instances separated by class. Otherwise, the measure is applied using the 
whole dataset. The following example shows how to compute the correlation 
between the attributes for the whole dataset:
```{r}
## Extract correlation using all instances together
mf.statistical(Species ~ ., iris, features="correlation", by.class=FALSE)
```
Note that the values obtained are different from the previous since the 
correlation between the attributes were computed over all the instances while
in the previous, the correlation were computed using the instances of the same
class.

The statistical measures return a list named by the requested measures. The 
`post.processing` methods are applied in these measures since they return 
multiple values. To define which them should be applied use the `summary` 
parameter, as detailed in the section **Post Processing Methods**.


## Discriminant

Discriminant meta-features are computed using the discriminant analysis. As it 
requires only numerical attributes, like statistical group, the categorical data 
are transformed to numerical. For instance, `cancor` and `discfct` are the first
 canonical discriminant correlations in the dataset and the number of 
discriminant functions normalized by the number of classes, respectively. To 
list the measures of this group use `ls.discriminant()`. The following examples 
illustrate these measures:

```{r}
## Show the the available general measures
ls.discriminant()

## Extract all discriminant measures
disc.iris <- mf.discriminant(Species ~ ., iris, features="all")

## Extract two discriminant measures
mf.discriminant(Species ~ ., iris, features=c("cancor", "discfct"))
```

The discriminant measures return a list named by the requested measures. Like
general group, the `post.processing` methods are not applied in these measures 
since they return simple values.


## Information Theoretical

Information Theoretical meta-features are particularly appropriate to describe
discrete (categorical) attributes, but they also fit continuous ones using a 
discretization process. These measures are based on information theory. For 
instance, `class.entropy` and `mutual.information` are the normalized entropy of 
the class and the common information shared between each attribute and the class 
in the dataset, respectively. To list the measures of this group use 
`ls.infotheo()`. The following examples illustrate these measures:

```{r}
## Show the the available infotheo measures
ls.infotheo()

## Extract all infotheo measures
inf.iris <- mf.infotheo(Species ~ ., iris, features="all")

## Extract two infotheo measures
mf.infotheo(Species ~ ., iris, 
            features=c("class.entropy", "mutual.information"))

```

The infotheo measures return a list named by the requested measures. The 
`post.processing` methods are applied in some measures since they return 
multiple values. To define which them should be applied use the `summary` 
parameter, as detailed in the section **Post Processing Methods**.

## Model based

## Landmarking

Landmarking measures are simple and fast algorithms, from which performance 
characteristics can be extracted. These measures include the accuracy of simple 
and efficient learning algorithms like Naive Bayes (`naive.bayes`) and 
1-Nearest Neighbor (`nearest.neighbor`). The following examples illustrate these
 measures:

```{r}
## Show the the available landmarking measures
ls.landmarking()

## Extract all landmarking measures
land.iris <- mf.landmarking(Species ~ ., iris, features="all")

## Extract two landmarking measures
mf.landmarking(Species ~ ., iris, features=c("naive.bayes", "nearest.neighbor"))
```

The accuracy extraction of these measures without a cross validation step can 
cause model overfitting in the data. Therefore the `mf.landmarking` function has
 the parammeter `folds` to define the number of `k`-fold cross-validation. The 
 following example show how to set this value:

```{r}
## Extract one landmarking measures with folds=5
mf.landmarking(Species ~ ., iris, features="naive.bayes", folds=5)
```

There are some measures interested to evaluate the linear separability 
(`linear.discriminant`) and the attribute information (`decision.stumps`). For 
thesse measures, multi-class problems need to be decomposed in binary 
classification problems. This package implemented two decomposition stragies: 
`one.vs.all` and `one.vs.one` strategy. The following code show how to use the 
decomposition strategies:


```{r}
## Extract one landmarking measures using one.vs.all strategy
mf.landmarking(Species ~ ., iris, features="linear.discriminant", 
               map="one.vs.all")

## Extract one landmarking measures using one.vs.one strategy
mf.landmarking(Species ~ ., iris, features"linear.discriminant", 
               map="one.vs.one")
```

Each one of these meta-features generate multiple values (by fold and/or 
attribute) and then it is post processed by the summary methods.

# Post Processing Methods
Several meta-features generate multiple values and usually the `mean` is used to
summary these values. In order to flexiblize it, the `mfe` uses the post 
processing methods to deal with multiples meta-features values. From a set of 
meta-feature values the post-processing summarizes the data using some 
descriptive statistic (resulting in a single value) or a distribution (resulting
 in multiples values).

The post processing meethods are setted using the parameter `summary`. Diferents 
methods can be used at the same time, for instance the default post processing 
are `c("mean", "sd")`. Any R method, similar to `mean`, can be used, as 
ilustrated in the following examples:
```{r}
## Apply several statistical measures as post processing
mf.statistical(Species ~ ., iris, "correlation", 
               summary=c("kurtosis", "max", "mean", "median", "min", "sd", 
                         "skewness", "var"))

## Apply quantile as post processing method
mf.statistical(Species ~ ., iris, "correlation", summary="quantile")
```

Beyond these R default methods, two additional post processing methods are 
available in the `mfe` package: `hist` and `non.aggregated`. The first computes 
a histogram of the values and returns the frequencies of in each bins. The extra
parameters `bins` can be used to define the number of values to be returned. T
he parameters `min` and `max` are used to define the range of the data. The 
second is a way to obtain all values from the measure. The following code 
illustrate examples of the use of these post precessing methods:

```{r}
## Apply histogram as post processing method
mf.statistical(Species ~ ., iris, "correlation", summary="hist")

## Apply histogram as post processing method and customize it
mf.statistical(Species ~ ., iris, "correlation", 
               summary="hist", bins=5, min=0, max=1)

## Extract all correlation values
mf.statistical(Species ~ ., iris, "correlation", summary="non.aggregated", 
               by.class=FALSE)
```

It is also possible define an user's post processing method, like this:
```{r}
## Compute the absolute difference between the mean and the median 
my.method <- function(x, ...) abs(mean(x) - median(x))

## Using the user defined post processing method
mf.statistical(Species ~ ., iris, "correlation", summary="my.method")
```

# Summary
In this paper the `mfe` package, aimed to extract meta-features from dataset, 
has been introduced. The functions supplied by this package allow both their 
use in MtL experiments as well as perform data analysis using characterization 
measures able to describe datasets. Currently, six groups of meta-features can 
be extracted for any classification dataset. These groups and features represent
the standard and the state of the art characterization measures. 

The `mfe` package was designed to be easily customized and extensible. The 
development of the `mfe` package will continue in the near future by including 
new meta-features, group of measures and supporting regression problems.
